\chapter{Baseline}
\label{ch:baseline}

Before we can start analyzing any data or to do any algorithm research, we need to clarify what exact data we want to process and what results we want to gather. In this chapter we explain the preprocessing of the data from the collection from the Google server over normalization up to the clean up. Then we craft a precise description of the what we think similarity is in our context.



\section{Data}
\label{sec:baseline:data}

\begin{figure}
    \centering
    \input{figures/clean_up_sums.tex}
    \caption[Data pruning breakdown]{Relative amount of pruned data per letter}\label{fig:clean_up_sums}
\end{figure}

As for most data analysis tasks, the raw input data is not suitable for direct information gathering. To clean up the data we are looking for an efficient and deterministic way. To do so we perform the following clean up steps:

\begin{enumerate}
    \item download and parsing: Retrieve the data from the Google servers and parse the data for the first time. This step results in a list of all $n$-grams within the data set but does not, for efficiency reasons, store the actual time series data.
    \item string filtering: Remove strings from the list of $n$-grams that contain certain characters. This removes punctuation and other unwanted entries.
    \item string normalization: At this point we handle the fact that semantically identical Unicode strings might be encoded in different ways.
    \item word normalization: For our application we ignore different word attributes like tenses and pluralization. This results in an unambiguous list of $n$-grams.
    \item pruning: the Google data set contains a lot of very rare entries that do not contain enough valuable information. At this point, we strip certain parts based on a time frame as well as statistical significance. This is the first time that the actual time series data is required. This delayed parsing removes a lot of overhead during the early stages of the pipeline.
\end{enumerate}

Note that swapping the string-based steps does not lead a different output. \autoref{fig:clean_up_sums} show how much data is pruned by every step for the $1$-gram data set. The exact numbers can be found in \autoref{tab:clean_up_numbers_1} and \autoref{tab:clean_up_numbers_2}. Naturally there are some differences between the different starting characters of the $n$-grams but as far as we can tell no anomalies can be observed.

If not stated otherwise, all figures and explanations in this work are derived from the $1$-gram data set.


\subsection{Download and Parsing}
\label{ssec:baseline:data:download}
As a first step we download the raw data from the Google server. A complete list of download URLs can be obtained here:

\url{https://storage.googleapis.com/books/ngrams/books/datasetsv2.html}

We use the files of version \code{20120701} listed under \enquote{American English}, with the exception of:

\begin{itemize}
    \item files with $n$-grams that for sure contain word types instead of types and therefore will be dropped\footnote{Does not apply to the $1$-gram data set.}: \code{\_ADJ\_}, \code{\_ADP\_}, \code{\_ADV\_}, \code{\_CONJ\_}, \code{\_DET\_}, \code{\_NOUN\_}, \code{\_NUM\_}, \code{\_PRON\_}, \code{\_PRT\_}, \code{\_VERB\_}
    \item files containing $n$-grams with numbers: \code{0}, \code{1}, \code{2}, \code{3}, \code{4}, \code{5}, \code{6}, \code{7}, \code{8}, \code{9}
    \item punctuation data: \code{punctuation}
    \item characters which do not belong to the English alphabet: \code{other}
\end{itemize}

The data collection and processing is described in~\cite{Google_nGrams}. The raw text data, which was downloaded from the Google servers, is mangled by a high-performance C++ implementation to speed-up the upcoming steps. During the first steps, only the $n$-gram content is required without any knowledge of the actual time series data. We exploit this fact and delay the actual parsing and storage of the time series until the data is required and focus on the handling of the string data as long as possible.


\subsection{String filtering}
\label{ssec:baseline:data:filter}
It seems that the $n$-grams that Google extracted does not only contain pure words but also numbers, punctuation characters and word classes. To simplify storage of the strings and because our users are not interested in searching for whole word types, we filter out all entries that contain characters of the following class:

\code{\[\_.\slash{},:;!?\textbackslash'"\#()<>=+*\{\}0-9\]}

We have found this character set sufficient for our work, but want to point out that there this is a parameter that depends on the concrete application. For example, we drop $n$-grams that contain the combined words like \enquote{It's}. To deal with this type of content, the pipeline needs to be extended to split words into a normalized form. In the following work we assume that all parts of the $n$-gram are atomic words.

\subsection{String Normalization}
\label{ssec:baseline:data:snorm}
Since the data the $n$-grams are described by Unicode strings, there may exist string describing the same content. We apply NFKC normalization as described in~\cite{unicode8annex15} to solve this issue. Furthermore we lowercase all inputs with respect to the Unicode standard. If this procedure will lead to duplicate $n$-grams, they are joined by adding the corresponding time series data.

Notice, that the lowercase transformation may not work as expected for other languages than English since it may lead to a loss of important information.


\subsection{Word Normalization}
\label{ssec:baseline:data:wnorm}
The words forming the $n$-grams exist in multiple variants, e.g., different terms for verbs or singular and plural form for nouns. Naturally the resulting time series are very similar and do not contain valuable information, neither for our algorithms nor for a human analyst. We solve this issue by applying the WordNet lemmatizer (\cite{wordnet}) and afterwards the snowball stemmer (\cite{porter2}) to all words. In case of same output $n$-grams the related time series are added again.

As for the former transformation step, be aware of the language problem. Other languages may require other word normalization techniques. Also, there may be more advanced techniques that exploit the knowledge of the entire $n$-gram instead of single words.

We decided not to run any OCR (optical character recognition) error recognition since we are not aware of any general purpose approach that does not transform rare words or names like a na\"{\i}ve spell checker would do.


\subsection{Pruning}
\label{ssec:baseline:data:prune}
The original data sets contains a lot of very rare $n$-grams that, in our opinion, do not provide enough statistical information to be considered during the further analysis. The same applies to all $n$-grams for the early years that are contained in the data.

To handle the first case, we have decided to drop all $n$-grams where the related match count time series is too small, or in mathematical terms where $\sum_{1753 \leq y \leq 2008} v_{0, y} < 1000$ ($v_0$ is the \enquote{match count}) applies. This threshold is, as many other parameters, an application-specific one. For us it was a trade-off of eliminating noise and keeping some very specific entries which might be required to answer some questions in the field of philosophy.

To eliminate the second source of irrelevant information we decided to only use the last \num{256} years of the time series data. The selection of the time range of \num{256} years has other advantages apart from the pure pruning. Since it is a power of \num{2} it is easier to apply many transformations (e.g., Fourier or Discrete Wavelet Transformation) to it without the need to think about and justify additional edge case handling.

Please note that this is the first step where the actual time series content is required. Therefore, it is also the first step where we parse the time series data for all $n$-gram strings that survived up to this point.


\section{Similarity}
\label{sec:baseline:sim}
The idea of similarity depends on the concrete application. The simplest one, which is obviously not suitable, is to just take the total count of $n$-grams over a time period and calculate the absolute distance to each other. Another idea would be to treat the time series as a high-dimensional vector and use the Euclidean distance as a description of how similar the different series are. For our application it was not clear beforehand, which precise mathematical definition is suitable. It should be justified by two parts. The first one is a purely data-driven approach. Because we want to overcome the subjective guessing described in the introduction, we want a similarity metric which is based on the actual time series data and nothing else. The second part is that the results should be sane in a semantically sense. So for $n$-grams that are similar there should, from a user point of view, also be a good reason to trait them as such.

Notice that similarity is a function which describes a metric while not being mathematically correct. We guarantee non-negativity and symmetry but the method we come up with does not provide identity of indiscernible and subadditivity.

In this section we first discuss, which time series we want to consider as similar. We want to explore, which $n$-grams are used together. Because there might be a normal usage of certain words or phrases in the day to day life, interesting data might be hidden by an overall large number and therefore interesting changes in usage patterns might not be discovered when total numbers of mentions are compared. Consider the following example: $n_1 \Rightarrow n_2$ but not vice versa and $n_2$ is already used in many books but $n_1$ is a new $n$-gram at a certain point of time. This implication cannot be found by comparing total numbers, because $n_2$ might be way larger than $n_1$ but the implication will lead to similar changes in the overall time series. So we seek to find similar structure instead of similar usage numbers.

\begin{definition}[Query]
    A query is a time series for which the user (or some program) wants to know the nearest neighbors for. By writing \enquote{some word} we refer to the time series that belongs to the normalized $n$-gram or the $n$-gram itself, depending on the context. The plural form queries is used to express multiple starting points for a nearest neighbor search, either when generating statistics or when calculating the distance to multiple time series.
\end{definition}

Furthermore we limit our analysis to the \enquote{match count} because it seems to be more useful for later research than the less fine grained \enquote{unique book count}.

To derive meaningful similarity, we will now explain the reason and execution of the different steps done for the metric calculation, which are executed in the following order:

\begin{enumerate}
    \item normalization: since the book counts grow exponentially but at the same time we want to ensure independent preprocessing of the single time series, we do a $\log(x + 1)$ transformation
    \item smoothing: to eliminate noise for the next step\footnote{This step does have its own subsection but is explained during the discussion of the gradient calculation.}
    \item gradients: we seek for structural changes withing the time series, not similar counts, so we calculate the distances of gradients instead of the original time series
    \item Dynamic Time Warping: time series can include small delays we want to compensate them before doing the actual distance calculation
    \item ranking: similarity can be used to derive a set of nearest neighbors; we explain how to do so, which problems occur due to noise and present one possible way to automatically select the size of the set of nearest neighbors
\end{enumerate}


\subsection{Normalization}
\label{ssec:baseline:sim:norm}

\begin{figure}
    \centering
    \input{figures/ngrams_ex_total.tex}
    \caption[Time series plot, absolute amount]{Total counts of example $n$-grams}\label{fig:ngrams_ex_total}
\end{figure}

\begin{figure}
    \centering
    \input{figures/ngrams_hist.tex}
    \caption[Histogram of time series data, absolute amount]{Histogram of all time series}\label{fig:ngrams_hist}
\end{figure}

In \autoref{fig:ngrams_ex_total} you can observe the following: the words \enquote{without}, \enquote{area} and \enquote{death} share the global low around 1945 while the word \enquote{war} does not. On the other hand you can see the same structure (peak and then depression) between 1961 and 1977 for \enquote{war} and \enquote{without}. Another feature that can be observed is the fact that the time series data is not limited in its value and that they behave exponentially, as shown in \autoref{fig:ngrams_hist}. Exponential growth is also common amongst this area and our natural environments (\cite{exp_growth1,exp_growth2}). To be able to determine a proper similarity between different time series, this scaling should be eliminated. There are two possible ways: applying a logarithm or normalizing every point in time by using a factor shared amongst all series.

\begin{figure}
    \centering
    \input{figures/ngrams_sums.tex}
    \caption[Plot of time series sum]{Absolute sum of all $n$-gram time series}\label{fig:ngrams_sums}
\end{figure}

\begin{figure}
    \centering
    \input{figures/ngrams_ex_relative.tex}
    \caption[Time series plot, relative amount]{Relative counts of example $n$-grams}\label{fig:ngrams_ex_relative}
\end{figure}

\autoref{fig:ngrams_sums} shows the foundation of such a factor. The problem with this approach is that time series with an overall huge impact on the sum will also influence the normalized results of small time series heavily. In other words: rather than symbolizing a straight growth, the sum itself has a structure, which then will influence the normalized result. An example result of this normalization is shown in \autoref{fig:ngrams_ex_relative}. It can be observed that the $n$-gram \enquote{war} seems to grow heavily during the time around the world wars. The reason for this is that the overall number of publications decreased during this time except for war-related topics. As explained, the structure of the time series sum itself now results in a new feature of the \enquote{war} data.

\begin{figure}
    \centering
    \input{figures/ngrams_ex_log.tex}
    \caption[Time series plot, $\log$]{$\log$ counts of example $n$-grams}\label{fig:ngrams_ex_log}
\end{figure}

\begin{figure}
    \centering
    \input{figures/ngrams_lhist.tex}
    \caption[Histogram of time series data, $\log$]{Histogram of $\log(x + 1)$ of all time series}\label{fig:ngrams_lhist}
\end{figure}

So we choose to apply $\log(x + 1)$ to all values for normalizing. Note the \num{1} that was added because the range of possible input values starts at \num{0}. The results are shown in \autoref{fig:ngrams_ex_log} and the corresponding histogram is shown in \autoref{fig:ngrams_lhist}. There is a small gap in the histogram, which is the bin $(0,0.5]$, which before transformation is $(0,0.65]$ and therefore no count values can fall into that range. Overall the results look way better distributed. It can also be observed that the later years are smoother than the beginning of the time series. This is due to the fact that larger values counts are, in relative means, less noisy than smaller counts.


\subsection{Gradients}
\label{ssec:baseline:sim:grad}

\begin{figure}
    \centering
    \input{figures/ngrams_ex2_fitavg.tex}
    \caption[Time series plot, fit using AVG]{Example $n$-grams, fit using AVG}\label{fig:ngrams_ex2_fitavg}
\end{figure}

\begin{figure}
    \centering
    \input{figures/ngrams_ex2_grad.tex}
    \caption[Time series plot, gradients]{Gradients of example $n$-grams}\label{fig:ngrams_ex2_grad}
\end{figure}

As \autoref{fig:ngrams_ex2_fitavg} shows, there are cases where time series look very similar but are difficult to fit by using a linear transformation. Therefore, a simple $p$-distance is not sufficient for as a distance measure. We decided to fit the gradients instead. An example is shown in \autoref{fig:ngrams_ex2_grad}. We use the $\Delta = 1$ gradient of $\log(x + 1)$. Now time series with similar structure have a low distance. The following two subsections will explain two additional tunings we made to make this distance more meaningful.

\begin{figure}
    \centering
    \input{figures/ngrams_ex2_grad_smooth.tex}
    \caption[Time series plot, smoothed gradients]{Gradients of example $n$-grams, smoothed with $\sigma = 1$}\label{fig:ngrams_ex2_grad_smooth}
\end{figure}

\begin{figure}
    \centering
    \input{figures/smoothing_frequencies.tex}
    \caption[Frequency plot for smoothing]{Frequency distribution (mean + standard deviation) for different smoothing levels}\label{fig:smoothing_frequencies}
\end{figure}

Since calculating the distances of noisy gradients may lead to useless results, we smooth the time series using a Gauss kernel with $\sigma = 1$ before deriving the gradient. An example is shown in \autoref{fig:ngrams_ex2_grad_smooth}. The reason we can use a Gauss kernel, which takes past and future values into account, for a time series here is the fact that the data we are transforming is known beforehand. This is different to most other scenarios where smoothing techniques like EWMA (exponentially weighted moving average) are required. Frequency distributions for different smoothing levels are shown in \autoref{fig:smoothing_frequencies}. The non-smoothed data shows quite high amplitudes in higher frequencies ranges, which is uncommon for most time series data sets. The chosen smoothing of $\sigma = 1$ reduces noise while $\sigma = 2$ leads to a heavy loss of information. The overall pattern of fast switching gradients will also lead to some other problems, e.g.\ in \autoref{sec:baseline:speed}. The similarity of the $1$-gram \enquote{kiss} is clearer now. As expected, the time data does not quite fit the other example series. This is intended since the underlying data also has a slightly different structure.


\subsection{Dynamic Time Warping}
\label{ssec:baseline:sim:dtw}

\begin{figure}
    \centering
    \input{figures/ngrams_ex2_extrema.tex}
    \caption[Extrema of time series]{Extrema of gradients of $\sigma = 2$ smoothed example time series}\label{fig:ngrams_ex2_extrema}
\end{figure}

Now we have reached a state where we could just calculate the quadratic distance of the smoothed time series data. The question is if this obvious approach is sane. If would do so, we would completely ignore the fact that events that influence one time series have an delayed effect on others. Someone might wonder if this effect occurs in our data set. Take a look at \autoref{fig:ngrams_ex2_extrema} where we plot the extrema points of the smoothed data. While all four time seres have the same structure, their smoothed gradients do not align perfectly. To calculate a meaningful distance, we use DTW (Dynamic Time Warping)\footnote{We do not cite a publication here since it is a commonly known algorithm and the only online publisher of the assumed original publication wants you to pay $>$\SI{30}{\$} after $>$\num{45} years.} with an Euclidean distance. The usage of gradients as an input of Dynamic Time Warping follows the idea presented in~\cite{DDTW}. To only allow a warping up to a certain distance we use Sakoe-Chiba Band from~\cite{sakoe} of radius $r$. This prevents the DTW algorithm from stretching the time series too heavily which would result in semantically insane results and therefore would contradict our goals we have for our similarity.


\subsection{Ranking}
\label{ssec:baseline:sim:rank}

It seems obvious to use the DTW distances to rank nearest neighbors. There are two major problems with that approach.

\begin{figure}
    \centering
    \input{figures/dists_hist.tex}
    \caption[Histogram of distances]{Histogram of distances to example $1$-grams}\label{fig:dists_hist}
\end{figure}

First the distribution of the distances differs heavily from query to query as shown in \autoref{fig:dists_hist}.

\begin{figure}
    \centering
    \input{figures/dists_sorted.tex}
    \caption[Sorted distances, with cutoff points]{Distance of the first \num{100} neighbors of example $1$-grams, without the first element, which is the query itself}\label{fig:dists_sorted}
\end{figure}

Second it is not clear how many nearest neighbors should be taken into account, especially when designing user-facing applications. To solve this problem we sort all neighbors by their distance and only take rank $1$ to $\frac{m}{2}$ into account, with $m$ being the size of the set of all $n$-grams. This ignores the query itself with its distance of \num{0} and prunes the second half of the sorted list because there are usually many outliers at the end of the spectrum. Then we normalize the distances:

\begin{equation}\label{eq:dists_norm}
    d_{n,i} = \frac{d_i - d_1}{d_1}
\end{equation}

Here $d_i$ is the distance of the neighbor with rank $i$, starting at \num{0} with the query itself. The result is shown in \autoref{fig:dists_sorted}. Now we search for the index that clearly cuts the group of the nearest neighbors from the rest of data. To do so we take into account the following observation: the derivative of the function of sorted nearest neighbors is usually decreasing and is then suddenly increasing again, isolating a dedicated group of nearest neighbors. We use this as a cutoff-point. The mathematical term is:

\begin{equation}\label{eq:dists_cut}
    i_\text{cut} = 1 + \argmax_{i=1}^\frac{m}{2} \left( \Delta_1 \left( \frac{\Delta_1(d_n)}{d_n} \right) \right)_i
\end{equation}

Here $\Delta_1$ calculates the gradient with distance \num{1}. Notice that we normalize the first derivative since we want to compare relative changes from a certain neighbor to the next one. This normalization is always possible due to the sorting of neighbors by rank. The second derivative can be positive and negative and therefore normalization is not desired and also would not have a semantical meaning.

That approach might seem to be slightly artificial and indeed its a more phenomenological metric instead of a mathematically proven approach. The main reason for this is that we want to keep the index as small as possible to enable users to understand the content of the list. If you try to use the set of nearest neighbors as an input for another algorithm you may want to choose another method.

Keep in mind that our method requires sorting of all neighbors while na{\"\i}ve $k$ nearest neighbors search requires only partial sorting. For a bruteforce search this should not be a problem since calculating the DTW distances dominates the execution time but it might be a problem when handling overly large data sets. Also, it renders index techniques useless since the algorithm cannot ensure to draw enough data from the index to reach the cutoff point. Therefore, we do not use this technique during all evaluations.

We also want to point out that this method is very instable. Small changes in the input data, e.g.\ by noise introduces by compression, will quickly lead to different results.



\section{Index-based Speed-up}
\label{sec:baseline:speed}

One of the fundamental problems of large-scale DTW-based similarity searches is that, without further preparation, a linear scan is required. In preparation of further research and usage of $5$-grams, we want to avoid this and cut the runtime complexity to $\mathcal{O}(\log{n})$. While doing so we still want to archive exact results, at least for our baseline. We decided to implement~\cite{LB_Keogh}, which is a combination of an R-tree index (\cite{rtree}) and early sorting and pruning by utilizing bound checks.

\begin{figure}
    \centering
    \input{figures/dtw_index.tex}
    \caption[DTW index efficiency per resolution]{DTW index efficiency, per resolution, $r$ is \num{10}}\label{fig:dtw_index}
\end{figure}

\begin{figure}
    \centering
    \input{figures/dtw_index2.tex}
    \caption[DTW index efficiency per $r$]{DTW index efficiency, per $r$, resolution is \num{64}}\label{fig:dtw_index2}
\end{figure}

A problem with this type of index is that the structure of the indexed data is different than the one tested in the publication. Our rather short data (in terms of measured points) contains more entropy and is more jittering than the time series intended by the paper. Therefore, we require more dimensions to get a tree that at least filters out some candidates as shown in \autoref{fig:dtw_index}. We need at least \num{16} dimensions to get any effect from the \code{LB\_PAA} and we do not even see any real filtering effect from the tree structure itself\footnote{the amount of received time series is more than \SI{95}{\percent}}. So the tree structure only provides a sorting of the time series. That results in a very memory-inefficient data structure. Remember that for every dimension, the R-tree stores two data points, a minimum and a maximum. So we need to store at least \SI{12.5}{\percent} of the actual data size as an index to get a any effect and still have a linear complexity. The high dimensionality renders the tree performance and memory management nearly useless, as also found by~\cite{rtree_highdim}. The situation depends on the warping radius as shown in \autoref{fig:dtw_index2}. For non-trivial values nearly the entire data is fetched from the index. Because all of this is a general problem of R-tree-based algorithms, we did not implement the improvements suggested by~\cite{LB_Improved}.



\section{Alternatives}
\label{sec:baseline:alt}

We want to point out that the baseline we have selected is not the only possible one. Other teams may come up with other setups depending on their definition of \enquote{similarity}. Our choices depend on the application we want to use the algorithms for and understanding of the humans operating these applications. But even with the exact same requirements it may still make sense to choose another baseline. It is important to keep in mind that the choices made during this process may lead to different results and that this might affect research outcomes of people using the similarity results to explore the data and to justify the results of their analysis.
