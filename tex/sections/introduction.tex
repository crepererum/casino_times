\chapter{Introduction}
\label{ch:Introduction}

Books are, even in times of blogs and electronic publications\footnote{This is 2016. It is likely that this will change.}, the most important way of to publish fictional and non-fictional stories, reports, education material, well-collected scientific material and lifetime achievements. The writing withing these books is material created at a certain point of history withing a specific society under concret, but not always known, circumstances. This is reflected in the usage of grammar and vocabulary. This reflection can be seen when reading a specific book, but it could also seen in a statistical way when analyzing many books. A major barrier is that someone needs to read these books and at the same time is activly looking for specific patterns, until recently. Google as one of the big companies that handle data was able to collect the content of a variety of books within their Google Books\footnote{\url{https://books.google.com/}} project. They than analyzed the content and collected statistical information about chains of words, called n-Grams (\cite{Google_nGrams}). The information is presented as time series which store the usage count of specific n-Grams over years. This enables people to check assumptions about how words were used over time and they are able to use these information to conclude about history of concrete phrases and abstract concepts.

A major question that arises when analyzing single words or n-Grams is if they are influenced by others or if there is any connection between particlar groups of words and phrases. There are two issues when someones tries to answer this question. The first one is to find a scientific measure of \enquote{influence}. Without such concrete way people will be affected by confirmation bias which makes most of them rate what they see depending on what they expect. The second one is two come up with ideas which candidates could have a connection. People, no matter of which profession, have a framed, subjective view on the world and therefore are unlikely to start their research with unexpected result. The existence of both issues hinders researchers to apply the scientific method to some fields and let them rely on expert knowledge and skills.

In this work, we solve the first problem and the second one partly. We provide and discuss a metric of similarity which also enables users to only take parts of the whole time series into account. This may not be the only we to describe similarity and we want to point out that it is important to check wether our proposal is suited for your work before applying it. Naturally other fields of application may require small changes or totally different approaches on how to describe connections between time series. The similarity measure directly leads to a way of finding similar n-Grams to a given query. The reason why this solves the framing problem only partly is that a user still needs to start with a query and because a na{\"i}ve search for neighbors is too slow when using the complete data set that Google provides.

To solve the second issue completly we need to speed-up the lookup-process. This issue here is that, compared to other time series data sets, the Google n-gram data is special which makes it unsuited for other indexing approaches. Therefore we explore our own way on how to store the time series data in a way that exploits similarities between them. This is done by transforming the data and then look for parts which can be shared between the transformed instances. It leads to an compression effect and speeds up query processing in a way that enable others then simple single-n-Gram queries. Also this enable researcher with low computational power and storage\footnote{compared to companies like Google} to process the whole data set of $1$ to $5$-grams.

This work will first set a baseline by explaining data mangling and clean-up, the choice of a similarity measure and the reason why indexing this kind of data does not work with known methods. We then will present our method from its foundation to a concrete algorithm and optimizations as well as reasons why our original idea works worse than expected. Afterwards we will discuss implementation details and will run a full analysis and comparison with the baseline. At many points we present alternatives to our approach which we encourage the reader to take serious. We conclude this work with possible future research topics and an outlook on the benifits of data-driven research based on this type of data.
