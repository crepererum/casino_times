\chapter{Introduction}
\label{ch:Introduction}

Books are, even in times of blogs and electronic publications\footnote{This is 2016. It is likely that this will change.}, the most important way to publish fictional and non-fictional stories, reports, education material, well-collected scientific material and lifetime achievements. The writing within these books is material created at a certain point of history within a specific society under concrete, but not always known, circumstances. This is reflected in the usage of grammar and vocabulary. This reflection can be seen when reading a specific book, but it could also be seen in a statistical way when analyzing many books. A major barrier is that someone needs to read these books and at the same time is actively looking for specific patterns, until recently. Google as one of the big companies that handle data was able to collect the content of a variety of books within their Google Books\footnote{\url{https://books.google.com/}} project. They then gathered the content of all books and collected statistical information about chains of words, called $n$-grams (\cite{Google_nGrams}). The information is presented as time series, which store the usage count of specific $n$-grams over years. This enables people to check assumptions about how words were used over time and they can use this information to conclude about history of concrete phrases and abstract concepts. The data already helped other groups to extract valuable knowledge in linguistics (\cite{others1,others5,others7}), NLP for video descriptions (\cite{others2}), probability research (\cite{others3,others6}), investigation of cultural change (\cite{others4}) and to improve spell and grammar checking (\cite{languagetool}).

A major question that arises when analyzing single words or $n$-grams is if they are influenced by others or if there is any connection between particular groups of words and phrases. We have seen two important issues when someone's tries to answer this question. The first one is to find a scientific measure of \enquote{influence}. Without such concrete way people will be affected by confirmation bias, which makes most of them rate what they see depending on what they expect (\cite{cbias1,cbias2}). This effect is even stronger when we do not provide them tools that present candidates simultaneously so a sequential checking is required (\cite{cbias3}). The second one is two come up with ideas which candidates could have a connection. People, no matter of which profession, have a framed, subjective view on the world and therefore are unlikely to start their research with unexpected result (\cite{framing1,framing2}). The existence of both issues hinders researchers to apply the scientific method to some fields and let them rely on expert knowledge and skills.

In this work, we provide a possible solution to the first and a partly complete solution to the second problem. We explore, discuss and define what \enquote{similar} means in our context and how a metric can be designed that matches this semantical idea. In addition to comparing entire time series, we also ensure that users can only take subranges of the series into account, so queries can be limited to certain epochs. This may not be the only way to describe similarity and we want to point out that it is important to check whether our proposal is suited for your work before applying it. Of course other fields of application may require small changes or totally different approaches on how to describe connections between time series. The similarity measure directly leads to a way of finding similar $n$-grams to a given query. The reason why this solves the framing problem only partly is that a user still needs to start with a query and because a na{\"\i}ve search for neighbors is too slow when using the complete data set that Google provides.

To solve the second issue completely we need to speed-up the lookup-process. This issue here is that, compared to other time series data sets, the Google $n$-gram data, which needs to be transformed in order to enable the similarity calculation, contains a lot of high-frequency information and therefore indexing the data is prevented by the curse of dimensionality. This makes the data set special when comparing to other time series, e.g.\ the ones considered in~\cite{LB_Keogh}. Therefore, we need to explore our own way on how to store the time series data in a way that exploits similarities between them. This is done by transforming the data and then look for parts that can be shared between the transformed instances. It leads to a compression effect and speeds up query processing in a way that enable others then simple single-$n$-gram queries. Also, this enable researcher with low computational power and storage\footnote{compared to companies like Google} to process the whole data set of $1$ to $5$-grams.

Enabling fast search queries is not only an advantage for human users of the system. It also enables machines to gain information about the history of words and phrases and may enable novel techniques to visualize and analyze the data. The relationship of $n$-grams can then be expressed as pair-wise distance, as distance to carefully selected words, as graph with or without edge label or as multi-graph that takes time slices into account. The fast on-demand availability of the similarity data can be key to a whole new kind of research work and, without doubt, will lead to interesting results. To archive fast results within the constraints of the methods we choose, we will provide a state-of-the-art implementation which uses efficient memory management, clever pre-computation and modern CPU technology to provide results in the lowest possible time.

An important disclaimer: the $n$-gram corpus used for this work makes the assumption that every usage of an $n$-gram has the same weight. This was already criticized by other researchers (\cite{countbad}) and we are aware of this problem. On the other hand, this data set is the only publicly accessible one of this kind and size and since there are not alternatives, we do not have a real choice. In principle our techniques also work with weighted sums, which may be available in the future. We hope that large scale literature research gets easier and we urge stakeholders to establish a workable fair use policy and to build up a digital archive that includes works from a wide variety of publishers and provides a proper API\@. Right now we are limited to bypass copyright limitations and rely on the, surely not selfless, kindness of big companies.

This work will first explore prior work and will set this contribution into context. Then we create a baseline by explaining data mangling and clean-up, which includes important decisions regarding $n$-gram normalization and information pruning. We then illustrate the choice of a similarity measure and why this leads to the curse of dimensionality, the reason why indexing this kind of data does not work with known methods. Afterwards, we will present our method from its foundation to a concrete algorithm and optimizations as well as reasons why our original idea works worse than expected. Then, we will discuss implementation details and will run a full analysis and comparison with the baseline. At many points we present alternatives to our approach, which we encourage the reader to take serious. We conclude this work with possible future research topics and an outlook on the benefits of data-driven research based on this type of data.
