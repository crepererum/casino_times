\chapter{Conclusion}
\label{ch:Conclusion}

After evaluating the baseline and our ideas we now want to conclude our work and give the reader an overview how it can be used.

We designed a good baseline which can be used for evaluation but also to start experimenting with the results. When limiting the research to $1$-grams, the nearest neighbors search is even suitable for interactive investigations and the results can be used by researchers of other fields to explore novel relationships. The data gathering and clean-up part can be extended by adding more sophisticated normalization techniques or error correction methods. Please keep in mind that our definition of similarity might differ from others and we are open to a discussion about it as well as new ideas.

A good description of our algorithm could probably be: \enquote{bad luck}. Our greedy method does not has a high enough chance to succeed, even with the tried improvements. On the hand: now we know, which approach is not working to solve the problem of fast similarity search and compression at the same time. We were able to collect a good amount of data which shows how the system behaves and which results can be expected. Without doubt, further research is required to make the n-gram data set accessible to humans and machines without the need to use super computers. We believe that the approach of merging trees is unlikely to work when relying on a leaf-to-root greedy approach only. Either a completely different strategy is required or another data representation which makes it easier to find shared information between the time series.

We hope that the implementation, that was carefully designed to be reusable, can be used by others to kickstart their projects. It might be even possible to use parts of the code for production systems. When used for research we hope that more code is published which would enable to recombine, test, reproduce and tune results of other groups.

For the future we are looking forward to see more data-driven research in more fields apart from mathematics and computer science as well as in journalism. We think that data sets like the one provided by Google can have a huge impact. The bad part of the current situation is that only large companies are able to produce such collections and that copyright restrictions and economic interests are a major issue for science. The availability of a large-scale digital library with a proper API would enable researchers to extract their own n-gram data sets with custom publication weights and pruning methods.

Apart from the concrete of Google data set we have used, our method might work for other data sets. We did not test this but may do in future publications. Especially for a smaller set of time series which have more sample points with lower frequencies the algorithm may result in better results. For this kind of data, the prior work needs to be re-evaluated of course.

A last and short word on the personal gain through this project: We learned a lot about this data set and how to implement and profile performant algorithms in general, how to structure implementations in a way that the are reusable and interceptable and as an not unimportant skill how to gather and visualize data for publications as well as how to create explanatory diagrams.
