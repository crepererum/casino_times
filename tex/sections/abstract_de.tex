\Abstract{}

Datengetriebene Forschung ist, wenn auch nicht neu, ein essentieller Trend des 21. Jahrhunderts. Dies gilt nicht nur für Disziplinen wie Naturwissenschaften und Wirtschaft sondern zunehmend auch für die Gesellschaftswissenschaften. Ein wichtiger Datenbestand in diesem Sektor ist das Google $n$-Gram Projekt. Es bietet Einblicke darüber, wie sich die Verwendung von Wortketten ($n$-Grams) über die Jahre hinweg entwickelt hat, gemessen am Bestand aller bei Google Books erfassten Bücher. Dies ermöglicht es, Beziehungen zwischen konkreten Wörtern aber innerhalb und zwischen abstrakten Konzepten herzustellen.

Ein grundlegendes Problem dabei ist, dass die Überprüfung dieser Beziehungen bisher häufig auf Vermutungen basiert und damit den Effekten des Framings und des Confirmation Bias unterworfen sind. In dieser Arbeit wird deshalb ein neutrales Maß entwickelt, um derartige Links zu messen und objektiv zu bewerten. Diese Metrik, welche während der Konstruktion immer wieder auf ihre Bedeutung hin untersucht wird, basiert auf dem sogenannten Dynamic Time Warping (DTW) der Gradienten der Zeitreihen.

Nach der Herleitung einer Ähnlichkeitsmetrik besteht nun die Notwendigkeit, diese schnell evaluieren zu können um von einem technischen System in kurzer Zeit die Nachbarn, in Bezug zu der Metrik, erfragen zu können. Es wird gezeigt, warum bisherige Verfahren an den Besonderheiten dieses Datensatzes scheitern. Danach wird ein System entwickelt, dass die Ähnlichkeiten zwischen Zeitreihen ausnutzt, um gleichzeitig einen Index für die genannten Anfragen als auch eine Kompression der Daten zu ermöglichen. Dieser Ansatz basiert auf der Transformation der Daten in Haar-Wavelets und deren Darstellung als Baumstruktur. Die resultierenden Bäume werden dann in einem Greedy-Verfahren miteinander verschmolzen. Dieser Ansatz ist notwendig, da eine allumfassende Suche nach optimalen Vereinigungen eine zu hohe algorithmische Komplexität aufweist und deshalb unpraktikabel ist.

Es zeigt sich allerdings, dass das Greedy-Verfahren Schwächen hat und zu häufig lokal optimale aber global suboptimale Entscheidungen trifft. Es werden deshalb mehrere, nicht notwendigerweise erfolgreiche, Versuche unternommen, dieses Problem zu umgehen. Außerdem werden einige Alternativen vorgestellt, die stattdessen möglich sind.

Ein weiterer wichtiger Teil dieser Arbeit ist die performante Implementierung der Algorithmen. Dies wird in einem extra Kapitel erläutert.

Anschließend werden mehrere Test und Vergleiche bezüglich der semantischen Bedeutung des Ähnlichkeitsmaßes, der Qualität der komprimierten Daten, der Eignung des Ansatzes als Index-Struktur und der Performanz des Verfahrens im Vergleich zur naiven Bruteforce-Suche als auch einer bisher bekannten Index-Struktur durchgeführt.

Die Arbeit endet mit einer kurzen Zusammenfassung und Vorschlägen und Empfehlungen bezüglich weiterer Forschungsarbeiten auf diesem Gebiet.
