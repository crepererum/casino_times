\chapter{Algorithm}
\label{ch:algorithm}

In this chapter we explain our approach to compress the data and how we want to exploit this compression to find similar time series. To do so we first describe how we transform time series into tree structures, then we explain the general idea of information pruning. Afterwards we develop a method that instead of pruning of information relies on the merging of similar information packages. We then discuss different error metrics, detail how our construction can be used as an index and explain some technical improvements made to the algorithm. Finally we show which issues our approach has and how we tried to work-around these.



\section{Discrete Wavelet Transform}
\label{sec:algorithm:wavelet}

\begin{figure}
    \centering
    \input{figures/wavelet_tree_example.tex}
    \caption{Wavelet Tree Construction}\label{fig:wavelet_tree_example}
\end{figure}

First of all, we need to find a time series representation that enables us to compare different series on different time scales and different granularities. While the granularity requirement is met by a Fourier Transform, it is not sufficient for different time ranges. The reason for it is that the entire series is disassembled into frequencies that span the hole range. That makes it difficult to compare sub-ranges. So we chose a different transformation --- the Discrete Wavelet Transform under usage of the Haar Wavelet (\cite{Haar}). A special property of this kind of transform is that it deconstructs a time series into a tree-like structure. Note that we only support time series which length is a power of two. A generalization might be archived by the application of~\cite{haar_tree_notwo}.

To explain how the tree decomposition works we first need to clarify some terms:

\begin{definition}[Superroot]
    Since in a wavelet tree every node only describes the difference between the left and the right subtree, the information about the absolute position of the time series is missing. To store this information we introduce a superroot, a node which is parent of the root and the only one which has exactly one child. The total position of the time series stored in that node is called anchor and has the symbol $a$.
\end{definition}

\begin{definition}[Depth, Level]
    A level is a node attribute measuring its distance from the superroot of the tree starting at \num{1}. So the root of the tree corresponds to level \num{1}.

    The depth of a tree defines how many levels it has. It can be calculated from the time series using the following formula:

    \begin{equation}\label{eq:d}
        d = \log_2 m
    \end{equation}
\end{definition}

Now the tree construction $s$ at the point in time $t$ can be described with:

\begin{equation}\label{eq:tree}
    s(t) = 2^\frac{d}{2} a + N_{1,1}(t)
\end{equation}

Here $N_{l, i}(t)$ describes the influence of a certain node in level $l$ and delta $i$ (since there are usually more than one nodes per level). The influence can be calculated with:

\begin{equation}\label{eq:node}
    N_{l, i}(t) = \begin{cases}
        \mathbb{1}_{t \in N_{l, i}} 2^\frac{d - l + 1}{2} x_{l, i} + N_{l + 1, 2i}(t) + N_{l + 1, 2i + 1}(t), & l \leq d \\
        0, & l > d
    \end{cases}
\end{equation}

Here $x_{l, i}$ is the coefficient stored in the node $N_{l, i}$ and $\mathbb{1}_{t \in N_{l, i}}$ has an affect to this point in time:

\begin{equation}\label{eq:matches}
    \mathbb{1}_{t \in N_{l, i}} = \begin{cases}
        1, & t \in 2^{d - l} \cdot (2i - 2, 2i - 1 ] \\
        -1, & t \in 2^{d - l} \cdot (2i - 1, 2i ] \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

An example is shown in \autoref{fig:wavelet_tree_example}. Keep in mind that this transformation is unambiguous and lossless.

Now we want to discuss what data is actually transformed. It turns out that using the unsmoothed, but logarithmic data works the best. The reason for this are the following: the $\log(x + 1)$ transformation avoids the handling of too large values and as shown before results in a better value distribution. Now smoothing would certainly simply later compression attempts of the wavelet data, but it already prunes information. That alone is not a huge deal, but it gets problematic when we modify the wavelet data. Altering the coefficients of the wavelet tree, e.g.\ by rounding, data conversion, tree pruning (see \autoref{sec:algorithm:pruning}) or tree merging (see \autoref{sec:algorithm:merge}); results in a unsmooth time series after restoring them from the wavelet data. So to use them afterwards a smoothing step is required. A pre-transform smoothing now would results in three steps of information loss while only smoothing after the information recovery makes parameters easier to tune and information loss easier to calculate.



\section{Tree Pruning}
\label{sec:algorithm:pruning}

\begin{algorithm}
    \KwData{Node $N$}
    \KwData{Error threshold $\epsilon$}
    \KwResult{Pruned Node $N$, resulting error increase}

    \SetKwFunction{GetCoefficient}{getCoefficient}
    \SetKwFunction{GetChild}{getChild}
    \SetKwFunction{IsPruned}{isPruned}
    \SetKwFunction{SetCoefficient}{setCoefficient}
    \SetKwFunction{SetPruned}{setPruned}

    \Begin {
        \tcc{Check if children are pruned}
        $p_l \leftarrow \IsPruned{\GetChild{$N$, $0$}}$\;
        $p_r \leftarrow \IsPruned{\GetChild{$N$, $1$}}$\;
        $p_c \leftarrow p_l \land p_r$\;

        \BlankLine
        \tcc{Calculate maximum error increase}
        $i \leftarrow 2^{d - l + 1}$\;
        $e \leftarrow |\GetCoefficient{N}| \cdot \sqrt{i}$\;
        \BlankLine
        \tcc{Prune if requirements are met}
        \eIf{$e < \epsilon \land p_c$}{
            $\SetPruned{N}$\;
            $\SetCoefficient{N, 0}$\;
            \Return{e}\;
        }{
            \Return{0}\;
        }
    }

    \caption{pruneNode}\label{algo:pruneNode}
\end{algorithm}

\begin{algorithm}
    \KwData{Tree $T$}
    \KwData{Error threshold $\epsilon$}
    \KwResult{Pruned Tree $T$}

    \SetKwFunction{GetLayer}{getLayer}
    \SetKwFunction{PruneNode}{pruneNode}
    \SetKwFunction{Shuffle}{shuffle}
    \SetKwFunction{Sort}{sort}

    \Begin {
        \For{$l=d$ \emph{\KwTo} $1$}{
            $N \leftarrow \GetLayer{T, l}$\;
            $\Shuffle{N}$\;
            $\Sort{N}$\tcp*[r]{optional}
            \For{n $\in$ N}{
                $\epsilon \leftarrow \epsilon - \PruneNode{n}$\;
                \tcp{optional break if $\epsilon \leq 0$}
            }
        }
    }

    \caption{pruneTree}\label{algo:pruneTree}
\end{algorithm}

The first attempt to reduce the amount of data is an obvious but simple approach: pruning of sub-trees that do not hold important data. This does not entirely reflect the information loss approach we will use for the final algorithm, but it is a good milestone to grasp the idea. The idea was also presented~in \cite{RACE}. A tree node is pruned if the following conditions are met:

\begin{enumerate}
    \item both children are pruned
    \item setting its coefficient to $0$ would not increase the error above a certain threshold
\end{enumerate}

So pruning just equals the zeroing of the coefficient, but it allows us to remove the node entirely and save storage space since it does not contain any additional information than both pruned children and the coefficient that does not have an influence anymore. The question is in which order nodes should be tried to be pruned since this algorithm is greedy method and will stop immediately after the threshold is reached. We propose to shuffle the nodes first to avoid a bias to the begin or end of the time series and then to sort the shuffled nodes starting the smallest coefficient. This makes it more likely to prune more nodes instead of just some with huge coefficients. Also the results are more stable as you can see while comparing \autoref{fig:ngrams_ex2_compression_unsorted} and \autoref{fig:ngrams_ex2_compression_sorted}. The algorithm is shown in \autoref{algo:pruneTree}.

\begin{figure}
    \centering
    \input{figures/ngrams_ex2_compression_unsorted.tex}
    \caption{Compression of example nGrams, without sorting/prioritizing}\label{fig:ngrams_ex2_compression_unsorted}
\end{figure}

\begin{figure}
    \centering
    \input{figures/ngrams_ex2_compression_sorted.tex}
    \caption{Compression of example nGrams, with sorting/prioritizing}\label{fig:ngrams_ex2_compression_sorted}
\end{figure}



\section{Tree Merging}
\label{sec:algorithm:merge}

An important thing to point out about tree pruning is that it does not exploit the fact that there are many time series which might have the same structure. The compression is purely executed on a single series. Tree merging uses a similar idea to tree pruning but with an important difference: instead of setting the coefficient to zero it will be set to a similar entry which is already present in the database (to which we come in a moment). Therefore, the subtree is not pruned but merged with another one. This can simply be done by altering the child pointer of a tree node. The error is calculated based on the difference of the time series resulting from the new tree and the original time series. A discussion of the error metric can be found in \autoref{sec:algorithm:error}.

Keep in mind to only merge nodes into others when their children are identical. Also we want to point out that we do not merge superroots. This would in theory be possible but we rarely see the case that two time series are similar enough to make that possible. Also it would complicate the implementation since we sometimes use the superroot to store additional information about the time series.

\begin{algorithm}
    \KwData{Node $N$}
    \KwData{Error threshold $\epsilon$}

    \SetKwFunction{FindNearestNeighbor}{findNearestNeighbor}
    \SetKwFunction{GetChild}{getChild}
    \SetKwFunction{GetIndex}{getIndex}
    \SetKwFunction{IsMerged}{isMerged}
    \SetKwFunction{NewMergeTask}{newMergeTask}

    \Begin {
        $m_l \leftarrow \IsMerged{\GetChild{$N$, $0$}}$\;
        $m_r \leftarrow \IsMerged{\GetChild{$N$, $1$}}$\;
        \If{$m_l \land m_r$}{
            $i \leftarrow \GetIndex{\GetChild{$N$, $0$}, \GetChild{$N$, $1$}}$\;
            $C \leftarrow \FindNearestNeighbor{i, N}$\;
            \If{$C$}{
                \Return{\NewMergeTask{$N$, $C$}};
            }
        }
        \Return{0}\;
    }

    \caption{createMergeTask}\label{algo:createMergeTask}
\end{algorithm}

\begin{algorithm}
    \KwData{Tree $T$}
    \KwData{Error threshold $\epsilon$}

    \SetKwFunction{CreateMergeTask}{createMergeTask}
    \SetKwFunction{Execute}{execute}
    \SetKwFunction{GetLeafes}{getLeafes}
    \SetKwFunction{GetParentOfNode}{getParentOfNode}
    \SetKwFunction{IsInErrorRange}{isInErrorRange}
    \SetKwFunction{IsEmpty}{isEmpty}
    \SetKwFunction{Push}{push}
    \SetKwFunction{RecalcError}{recalcError}
    \SetKwFunction{RemoveTop}{removeTop}
    \SetKwFunction{Shuffle}{shuffle}
    \SetKwFunction{Update}{update}

    \Begin {
        $q \leftarrow \varnothing$\;
        \For{$l \in \Shuffle{\GetLeafes{$T$}}$}{
            $\Push{q, \CreateMergeTask{$l$}}$\;
        }
        \BlankLine
        $E \leftarrow \RecalcError{T}$\;
        \While{$\neg \IsEmpty{$q$}$}{
            $t \leftarrow \RemoveTop{q}$\;
            \If{$\IsInErrorRange{$E$, $\epsilon$, $t$}$}{
                $\Update{E, t}$\;
                $\Execute{t}$\;
                $s \leftarrow \CreateMergeTask{\GetParentOfNode{$t$}}$\;
                \If{$s$} {
                    $\Push{q, s}$\;
                }
            }
        }
    }

    \caption{addTreeToIndex}\label{algo:addTreeToIndex}
\end{algorithm}

The construction of the database of known trees\footnote{To be precise: since the trees share common subtrees, they are not real trees anymore. We will continue to refer to them as trees since it is a simple and easy to understand term.} is solved as followed: the time series are added to the database in random order. One whole input tree of an entire time series is (partly) merged into the database, then the next one and so on. For the tree merging itself we changed from a strict bottom-up approach to a queue-based approach since that results in higher compression rates. The problem with the bottom-up approach is the following: it might be that there exist very similar subtrees which are cheap to merge but a strict level-ordered merging would result in prioritizing nodes next to the leaves of the tree. That is contradictory to our goal of merging as many subtrees as possible.

Still the queue-based approach needs to start at the bottom and tends to be bottom-up. This is useful in a sense that the similarity we have defined is based on the gradient. Gradients are more influenced by the lower layers of the wavelet tree and so we merge this information first. This will be helpful for using the merged trees as an index structure.

The final algorithm is shown in \autoref{algo:addTreeToIndex}.



\section{Error Metric}
\label{sec:algorithm:error}

The choice of the error metric, while being easy in many cases, opposes some questions in our case:

\begin{itemize}
    \item How fast can a possible implementation be, not only in terms of complexity but also in terms of constant factors due to vectorization and cache layout?
    \item How good can it justify the quality of the decompressed data recovered from wavelet tree?
    \item How well does the decompressed data it work with the later smoothing, gradient calculation and DTW\@?
\end{itemize}

We decided to try three different metrics during the process which are described here.


\subsection{Summerized linear distance}
\label{ssec:algorithm:error:linear}

The first metric, a linear absolute distance, is one which naturally occurs when looking for something easy to implement:

\begin{equation}\label{eq:e1}
    e_1\left(t, t'\right) = \frac{1}{m} \sum_{i=1}^m \left|t_i - t'_i\right|
\end{equation}

The upper bounds can directly be calculated during the tree merging process since the influence regarding location and amplitude of certain tree nodes are know a priory. So the upper limit is simply the sum of the re-adjusted differences of the original node and the merging partner:

\begin{equation}\label{eq:e1_up}
    \bar{e}_1\left(t, t'\right) = \frac{1}{m} \sum_{m \in \mathrm{merges}(t, t')} 2^\frac{d - m_l}{2} \left|m_x - m_y\right|
\end{equation}

Here $m_l$ stands for the level of the merged node and $m_x$ and $m_y$ for the coefficient of the original node and the merge partner. Notice that all merges are included in the sum. So even when a node gets merged after both children got merged, they children are included as well.

While the implementation is rather fast because guessing of the upper limit does not require any knowledge of the actual time series difference, it is not well suited for all applications. A compression using this metric can lead to many jumps within the decompressed time series. Also the metric tends to sacrifice huge local errors for small improvements of the overall compression rate.


\subsection{Summerized quadratic distance}
\label{ssec:algorithm:error:quadratic}

A way to put higher panilties on to huge local errors is a quadratic error function:

\begin{equation}\label{eq:e2}
    e_2\left(t, t'\right) = \frac{1}{m} \sum_{i=1}^m \left(t_i - t'_i\right)^2
\end{equation}

A drawback of this function is that the calculation requires the actual distance of the original time series and the compressed one:

\begin{equation}\label{eq:e2_up}
    \bar{e}_2\left(t, t'\right) = \frac{1}{m} \sum_{i=1}^m \bar{\delta}_i(t, t')^2
\end{equation}

Here $\bar{\delta}_i(t, t')$ is the upper limit of the distance between $t$ and $t'$ at the point in time $i$. This one can be guessed using a similar method like \autoref{eq:e1_up}.

\begin{equation}\label{eq:delta_up}
    \bar{\delta}_i\left(t, t'\right) = \sum_{m \in \mathrm{merges}(t, t')} \mathbb{1}_{m \in i} \cdot 2^{-\frac{d - m_l}{2}} \left|m_x - m_y\right|
\end{equation}

where $\mathbb{1}_{m \in i}$ describes the fact that the merge $m$ does affect the point in time $i$. Notice the minus in the exponent. Since that upper limit strictly increases with at least the amount of the real delta during each merge, the error metric will reach the user-provided limit faster than expected. So we use a hybrid approach during the merging procedure: do cheap guessing until we reach the error limit and then calculate the real error and use this as a base for the following sum-up operations. The number of recalculations increases when slowly approaching the error limit but it is still way faster than calculating the real error on every merge.


\subsection{Delta Range}
\label{ssec:algorithm:error:range}

Our last metric is an application specific one which is based on the following observation: Shifting the entire time series up or down (in terms of the $y$-Axis) does not affect the gradient. Therefore this should be allowed during the compression procedure. So we use the distance between the maximum of the delta and the minimum of the delta as an error function. Keep in mind that the delta itself is signed.

\begin{equation}\label{eq:e3}
    e_3\left(t, t'\right) = \max_{i=1}^m \left|t_i - t'_i\right| - \min_{i=1}^m \left|t_i - t'_i\right|
\end{equation}

This metric has the same drawback as the one presented in \autoref{ssec:algorithm:error:quadratic} but can guessed using a similar strategy:

\begin{equation}\label{eq:e3_up}
    \bar{e}_3\left(t, t'\right) = \max_{i=1}^m \left|\bar{\delta}_i(t, t')\right| - \min_{i=1}^m \left|\bar{\delta}_i(t, t')\right|
\end{equation}



\section{Tree as Index}
\label{sec:algorithm:asindex}

\begin{algorithm}
    \KwData{Nodes $N$}

    \SetKwFunction{Append}{append}
    \SetKwFunction{FilterDown}{filterDown}
    \SetKwFunction{GetChild}{getChild}
    \SetKwFunction{Insert}{insert}
    \SetKwFunction{IsEmpty}{isEmpty}
    \SetKwFunction{TraceDown}{traceDown}
    \SetKwFunction{TraceUp}{traceUp}

    \Begin {
        $D \leftarrow \varnothing$\;
        $U \leftarrow \varnothing$\;
        \For{$n \in N$}{
            $\Insert{U, n, 1, (a, b) \rightarrow ab}$\tcp*[r]{start with weight of $1$}
            \If{$\FilterDown{$n$}$}{
                $\Append{D, \GetChild{$n$, $0$}}$\;
                $\Append{D, \GetChild{$n$, $1$}}$\;
            }
        }
        \BlankLine
        \If{$\neg\IsEmpty{$U$}$}{
            $\TraceUp{U}$\;
        }
        \If{$\neg\IsEmpty{$D$}$}{
            $\TraceDown{D}$\;
        }
    }

    \caption{traceDown}\label{algo:traceDown}
\end{algorithm}

\begin{algorithm}
    \KwData{Nodes $N$}

    \SetKwFunction{FilterUp}{filterUp}
    \SetKwFunction{FindParents}{findParents}
    \SetKwFunction{FindSuperroots}{findSuperroots}
    \SetKwFunction{GetChild}{getChild}
    \SetKwFunction{HandleSuperroot}{handleSuperroot}
    \SetKwFunction{Insert}{insert}
    \SetKwFunction{IsEmpty}{isEmpty}
    \SetKwFunction{TraceUp}{traceUp}

    \Begin {
        $U \leftarrow \varnothing$\;
        \For{$(n, w) \in N$}{
            \If{$\FilterUp{$n$, $w$}$}{
                $P \leftarrow \FindParents{n}$\;
                \For{$p \in P$}{
                    $v \leftarrow \frac{w}{|P|}$\tcp*[r]{split weight}
                    $\Insert{U, p, v, (a, b) \rightarrow ab}$\tcp*[r]{accumulate weight per parent}
                }
                \BlankLine
                $S \leftarrow \FindSuperroots{n}$\;
                \For{$s \in S$}{
                    $\HandleSuperroot{s, w}$\;
                }
            }
        }
        \BlankLine
        \If{$\neg\IsEmpty{$U$}$}{
            $\TraceUp{U}$\;
        }
    }

    \caption{traceUp}\label{algo:traceUp}
\end{algorithm}

Besides providing compression, our tree construction also expresses similarity of time series in a way that it can be used as an index. To do so, we provide a generic tracing approach. Starting with the root of the query, we walk down the tree level by level to its leafs. At every level, we start trace back the nodes of the current level to all possible superroots. While the children during the trace down are unambiguous, the parents during the trace up approach are not, since subtrees can be shared by multiple trees and therefore can have multiple superroots. The two parts of the algorithm are also shown in \autoref{algo:traceDown} and \autoref{algo:traceUp}. Besides emitting the superroots using a callback, we also provide filtering of the trace paths and some kind of weight accounting. Using this accounting calculates the chance ending up at a certain superroot if multiple random walkers would start at all possible nodes at the start of the trace up phase. It can be used to limit the number of possible paths. The filters can also be used to limit the tracer to specific parts of the time series or to stop the trace down earlier. This can be a huge advantage since higher error threshold during compression lead to too many possible superroots of nodes in the levels near the leaf nodes.

Our experiments will implement the following filter scheme:

\begin{itemize}
    \item \code{filterDown}: drop branch if it leads to a time domain which were not requested for DTW; stop if a maximum depth is reached
    \item \code{filterUp}: drop branch if weight is below or equal a threshold
\end{itemize}

We leave an open call to find and apply more sophisticated filtering techniques.



\section{Optimizations}
\label{sec:algorithm:opt}

There are three technical details which help to improve the compression ratio even further. The first one is the reduction of precision used to the coefficients. Instead of using full \SI{64}{\bit} floating point numbers we use \SI{16}{\bit} floats, also referred as \enquote{halfs} as defined by~\cite{ieee_float}. This increases the error due to conversion and leads to a lower amount of merges, but increases the overall compression rate in terms of memory size. The smaller size of the memory representation compensates the higher error. Using \SI{16}{\bit} numbers turned out to be better than \SI{32}{\bit}. There might be the possibility to use even less memory by using bit packing and changing the representation to a non-IEEE-compliant data type.

The second improvement is made by changing the representation of the child pointers within the tree nodes. Because we store all trees within a single memory-mapped region and this region may appear at different offsets during different program executions, we use offset pointers instead of real pointers. These offset pointers store the offset from the pointer itself to its target. Because the target is always located within the same mapped region, the offset stays constant no matter which global position the region has. Originally we stored \SI{64}{\bit} signed integer values as offsets. We reduced this to \SI{48}{\bit} values which is enough to store \SI[parse-numbers = false]{2^{48 - 1}}{\byte} $=$ \SI{128}{\tera\byte}.

The last one is a change on how the time series are shuffled before added to the index. Instead of doing a complete random shuffle, we separate the set of time series into chunks and shuffle these while preserving the order within the chunks. This improves memory locality and therefore results in better IO performance. The results vary from case to case and can be slightly worse than a full shuffle. We set the chunk size to \num{16} which we found a good tradeoff.

A possible improvement closely related to reducing the size of the offset pointers is the removal of the null pointers stored in all leaf nodes. We did not implement this simply because it makes the code more complicated and also because there are not enough leaf nodes left after compression to justify this change.



\section{Weaknesses}
\label{sec:algorithm:weak}

\begin{figure}
    \centering
    \input{figures/weakness.tex}
    \caption{Compression per layer}\label{fig:weakness}
\end{figure}

The main problem with our approach is its greedy nature. As shown in \autoref{fig:weakness}, the merges usually do not reach very high levels. This is caused by the decreasing probability of merging with a node that belongs to the same large subtree. They further the merge process continues, the more likely it gets that somewhere in between there was a better node from a different subtree as a nearest neighbor. On we other hand we can not check all possible merges in all layers because the runtime complexity would grow exponentially with the number of indexed time series. So we to limit the algorithm to a subset of decisions.

\begin{figure}
    \centering
    \input{figures/compression_timeline.tex}
    \caption{Compression over the compression process}\label{fig:compression_timeline}
\end{figure}

\autoref{fig:compression_timeline} adds another detail to that problem. It shows how the compression rate developers when more data is added to the index. When using low error thresholds, the following can be observed: at the beginning, not enough data is present in the database so the first entries are not compressed very well. As more data is added, the compression rate improves. This improvement is limited by the fact that it becomes harder to find the right merges that allow to merge larger subtrees. When using higher error thresholds this becomes more obvious. Because the local merging is not limited by the threshold anymore, the choice of the right neighbor that allows to merge its parent as well plays a bigger role now. As a result the behavior flips and the results get worse the more data is added to the index. When using such high thresholds it would be better to compress the data in chunks instead of the whole data set as one piece.

While the following section explain some of the fixes we have tried, we came to the conclusion that it might require a totally different strategy to find larger merges.



\section{Failed Improvements}
\label{sec:algorithm:fail}

We tried different approaches to solve to increase the likelihood to merge with larger subtrees but did not succeed. This section explain the methods, the idea behind them and the results.


\subsection{Subtree Index}
\label{ssec:algorithm:fail:stindex}

To increase the chance that the greedy algorithm chooses merges that belong to the same subtree we tried the following method:

\begin{enumerate}
    \item select multiple nearest neighbors as candidates for a merge
    \item do this for a node which belongs to the same \num{3}-subtree (or larger ones) as well
    \item check which pairs of merges belong to the same subtree and priorize them
\end{enumerate}

This requires that we are quickly able to figure out which nodes have which parents, so we introduced a subtree and superroot index to do so. Also we need to query multiple neighbors instead of just one. The index structure is the same as used for the tracer, but since compression requires way more index requests than the normal index usage, that approach is to slow for that application. Altogether that renders the entire approach impractical and we abandoned it. Also the compression results only improved slightly and sometimes even got worse.


\subsection{FLANN}
\label{ssec:algorithm:fail:flann}

The main goal of our algorithm is to find the best matching subtree in terms of distance. To help the greedy algorithm to archive that task we tried to use FLANN (\cite{FLANN}) to merge subtrees of size \num{3} instead of single nodes. To do so we need to ensure that the coefficients stored in the tree nodes are pre-multiplied like the elements in \autoref{eq:e1_up}. It turns out that the huge amount of indices required renders this method impossible to implement since it is not as memory and time efficient as plain sorted arrays. The result is that the calculation never finishes. Surprisingly the partial compression results are not even better which might be due to the fact that merging entire subtrees tends to be less aggressive compared to handling single nodes.


\subsection{Random Boosting}
\label{ssec:algorithm:fail:random}

This improvement is similar to the one presented in \autoref{ssec:algorithm:fail:stindex}. We try to find better merges by observing more neighbors. This time we tried to gather a fixed amount of neighbors and select a random one with decreasing probability bound to the rank. Then we execute a complete subtree merge. This process is repeated multiple times and the best merge is kept. We also add a merge to the set where only the nearest neighbor is used to ensure that the greedy decision never is worse due to the random elements, at least as a local outcome for that specific time series. We ended up with a heavily increased calculation time and less than \SI{5}{\percent} improvements of the compression rate.


\subsection{DTW}
\label{ssec:algorithm:fail:dtw}

One of the main limitations of the current neighborhood search is that the we are only looking for nodes which correspond to the same time span. This contradicts the later usage of the data through Dynamic Time Warping. So we tried to decouple the tree structure and subtree merging from the actual time dimensions. We limited the search for the child nodes to a time-monotonic warping and reused the implementation of the existing DTW\@. For the non-leaf-nodes we did not apply any constraint since the same children policy already forces merges with the correct level and correct warping. The results of this technique were not convincing since the additional degree of freedom during the merging makes it even more unlikely to hit neighbors which belong to the same subtree. So the compression rates got worse and we dropped this approach.


\subsection{Timeless Index}
\label{ssec:algorithm:fail:timeless}

One idea that may arise during investigating the results of DTW is that the time aspect of the index structure might not be that important at all. We gave this a try and removed the constraint from the leaf nodes entirely so that there exist a one-dimensional index for them. The inner nodes and the root node are only constrained by the same-children-policy. The results are, without surprise, pretty bad. The reason for this is that now the chance of finding large subtree-merges is even lower since merges on the leaf-layer are purely picked based on the coefficient and without any consideration of the consequences for the parent layer.


\subsection{Pruning}
\label{ssec:algorithm:fail:pruning}

A possible reason for suboptimal decisions during the local node merging might be that coefficients can be quite noisy and therefore expose to many possible merges which do not really differ within their value. Because only the nearest neighbor is selected for a merge, this may result in insane results. We tried to solve this problem by introducing additional information pruning in three variants:

\begin{enumerate}
    \item before merging
    \item during merging
    \item after merging
\end{enumerate}

The pruning zeroes low significant parts of the floating point coefficients, bit by bit, so with increasing error. So the ratio of the information loss stays approximately constant no matter which size the coefficient has. The idea behind this is to engage the algorithm to ignore noise within the coefficient and to have less decisions to make. On the other hand this pruning also increases the error without resulting in any compression at all and is only useful for entries that will be added to the index afterwards. Sadly the increase of the error rate overweights the desired affect of helping the greedy algorithm and therefore this approach does not have a positive effect, in all three variants.

Keep in mind that this type of pruning does not decrease the memory requirements of the tree nodes since there is no fixed cutoff but rather a individual decision how much information should be kept. So to than decrease the node size by storing a shorter representation of the coefficient you would also need to store how much information got pruned which takes so much space that it eliminates its purpose. For technical reason it is also quite difficult to store nodes with sizes not fitting into hole bytes.


\subsection{Seeding}
\label{ssec:algorithm:fail:seed}

As shown in \autoref{fig:compression_timeline}, normally the first elements inserted into the database are not compressed very well. This is due to the fact that there do not exist enough possible subtrees to merge with. An idea to improve the initial compression rate is to insert some seeds into the database before starting compression. There are multiple possible types of seeds:

\begin{enumerate}
    \item randomly chosen or by metric selected elements of the dataset
    \item fixed value entries, e.g.\ zero
    \item time series generated by sinus/cosinus waves or combinations of them
    \item fractals
\end{enumerate}

While choosing elements of the dataset is considered cheating because we would add data to the database without counting it, the other methods would in general work because the description of the corresponding trees could be considered as part of the model and are fixed for all compressed data sets. The main issue is that someone needs to come up with a proper choice of this seeds and we were not able to figure out a proper way to do this.
