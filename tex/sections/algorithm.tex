\chapter{Algorithm}
\label{ch:algorithm}

\dots



\section{Discrete Wavelet Transform}
\label{sec:algorithm:wavelet}

\begin{figure}
    \centering
    \input{figures/wavelet_tree_example.tex}
    \caption{Wavelet Tree Construction}
    \label{fig:wavelet_tree_example}
\end{figure}

First of all, we need to find a time series representation that enables us to compare different series on different time scales and different granularities. While the granularity requirement is met by a Fourier Transform, it is not sufficient for different time ranges. The reason for it is that the entire series is disassembled into frequencies that span the hole range. That makes it difficult to compare sub-ranges. So we chose a different transformation -- the Discrete Wavelet Transform under usage of the Haar Wavelet (\cite{Haar}). A special property of this kind of transform is that it deconstructs a time series into a tree-like structure as shown in \autoref{fig:wavelet_tree_example}. Note that we only support time series which length is a power of two. A generalization might be archived by the application of \cite{haar_tree_notwo}.

Now we want to discuss what data is actually transformed. It turns out that using the unsmoothed, but logarithmic data works the best. The reason for this are the following: the $\log(x + 1)$ transformation avoids the handling of too large values and as shown before results in a better value distribution. Now smoothing would certainly simply later compression attempts of the wavelet data, but it already prunes information. That alone is not a huge deal, but it gets problematic when we modify the wavelet data. Alterning the coefficients of the wavelet tree, e.g. by rounding, data conversion, tree pruning (see \autoref{sec:algorithm:pruning}) or tree merging (see \autoref{sec:algorithm:merge}); results in a unsmooth time series after restoring them from the wavelet data. So to use them afterwards a smoothing step is required. A pre-transform smoothing now would results in three steps of information loss while only smoothing after the information recovery makes parameters easier to tune and information loss easier to calculate.



\section{Tree Pruning}
\label{sec:algorithm:pruning}

\begin{algorithm}
    \KwData{Node $n$}
    \KwData{Error threshold $t$}
    \KwResult{Pruned Node $n$, resulting error increase}

    \Begin {
        \tcc{Check if children are pruned}
        $p_l$ $\leftarrow$ $\neg n.\textrm{child\_l} \lor n.\textrm{child\_l}.\textrm{pruned}$\;
        $p_r$ $\leftarrow$ $\neg n.\textrm{child\_r} \lor n.\textrm{child\_r}.\textrm{pruned}$\;
        $p_c$ $\leftarrow$ $p_l \land p_r$\;
        \BlankLine
        \tcc{Calculate maximum error increase}
        $i$ $\leftarrow$ $2^{d - l + 1}$\;
        $e$ $\leftarrow$ $|n.\textrm{x}| \cdot \sqrt{i}$\;
        \BlankLine
        \tcc{Prune if requirements are met}
        \eIf{$e < t \land p_c$}{
            $n.\textrm{pruned}$ $\leftarrow$ $1$\;
            $n.\textrm{x}$ $\leftarrow$ $0$\;
            \Return{e}\;
        }{
            \Return{0}\;
        }
    }

    \caption{pruneNode}
    \label{algo:pruneNode}
\end{algorithm}

\begin{algorithm}
    \KwData{Tree $T$}
    \KwData{Error threshold $t$}
    \KwResult{Pruned Tree $T$}

    \SetKwFunction{GetLayer}{getLayer}
    \SetKwFunction{PruneNode}{pruneNode}
    \SetKwFunction{Shuffle}{shuffle}
    \SetKwFunction{Sort}{sort}

    \Begin {
        \For{l=d \emph{\KwTo} 1}{
            $N$ $\leftarrow$ $\GetLayer{T, l}$\;
            $\Shuffle{N}$\;
            $\Sort{N}$\tcp*[r]{optional}
            \For{n $\in$ N}{
                $t$ $\leftarrow$ $t - \PruneNode{n}$\;
            }
        }
    }

    \caption{pruneTree}
    \label{algo:pruneTree}
\end{algorithm}

The first attempt to reduce the amount of data is an obvious but simple approach: pruning of sub-trees that do not hold important data. This does not entirly reflect the information loss approach we will use for the final algorithm, but it is a good milestone to grasp the idea. A tree node is pruned if the following conditions are met:

\begin{enumerate}
    \item both children are pruned
    \item setting its coefficient to $0$ would not increase the error above a certain threshold
\end{enumerate}

So pruning just equals the zeroing of the coefficient, but it allows us to remove the node entirely and save storage space since it does not contain any additional information than both pruned children and the coefficient that does not have an influence anymore. The question is in which order nodes should be tried to be pruned since this algorithm is greedy method and will stop immidiatly after the threshold is reached. We propose to shuffle the nodes first to avoid a bias to the begin or end of the time series and then to sort the shuffled nodes starting the smallest coefficient. This makes it more likely to prune more nodes instead of just some with huge coefficients. Also the results are more stable as you can see while comparing \autoref{fig:ngrams_ex2_compression_unsorted} and \autoref{fig:ngrams_ex2_compression_sorted}. The algorithm is shown in \autoref{algo:pruneTree}.

\begin{figure}
    \centering
    \input{figures/ngrams_ex2_compression_unsorted.tex}
    \caption{Compression of example nGrams, without sorting/prioritizing}
    \label{fig:ngrams_ex2_compression_unsorted}
\end{figure}

\begin{figure}
    \centering
    \input{figures/ngrams_ex2_compression_sorted.tex}
    \caption{Compression of example nGrams, with sorting/prioritizing}
    \label{fig:ngrams_ex2_compression_sorted}
\end{figure}



\section{Tree Merging}
\label{sec:algorithm:merge}

An important thing to point out about tree pruning is that it does not exploit the fact that there are many time series which might have the same structure. The compression is purely executed on a single series. Tree merging uses a similar idea to tree pruning but with an important difference: instead of setting the coefficient to zero it will be set to a similar entry which is already present in the database (to which we come in a moment). Therefore, the subtree is not pruned but merged with another one. This can simply be done by alterning the child pointer of a tree node. The error is calculated based on the difference of the time series resulting from the new tree and the original time series. A discussion of the error metric can be found in \autoref{sec:algorithm:error}. Keep in mind to only merge nodes into others when their children are identical.

The construction of the database of known trees\footnote{To be precise: since the trees share common subtrees, they are not real trees anymore. We will continue to referr to them as trees since it is a simple and easy to understand term.} is solved as followed: the time series are added to the database in random order. One whole input tree of an entire time series is (partly) merged into the database, then the next one and so on. For the tree merging itself we changed from a strict buttom-up approach to a queue-based approach since that results in higher compression rates. The problem with the buttom-up approach is the following: it might be that there exist very similar subtrees which are cheap to merge but a strict level-ordered merging would result in prioritizing nodes next to the leaves of the tree. That is contridictary to our goal of merging as many subtrees as possible.



\section{Error Metric}
\label{sec:algorithm:error}

The choice of the error metric, while being easy in many cases, opposes some questions in our case:

\begin{itemize}
    \item How fast can a possible implementation be, not only in terms of complexity but also in terms of constant factors due to vectorization and cache layout?
    \item How good can it justify the quality of the decompressed data recovered from wavelet tree?
    \item How well does the decompressed data it work with the later smoothing, gradient calculation and DTW?
\end{itemize}

We decided to try three different metrics during the process which are described here.


\subsection{Summerized linear distance}
\label{ssec:algorithm:error:linear}

The first metric, a linear absolute distance, is one which naturally occurs when looking for something easy to implement:

\begin{equation}\label{eq:e1}
    e_1\left(t, t'\right) = \frac{1}{m} \sum_{i=1}^m \left|t_i - t'_i\right|
\end{equation}

The upper bounds can directly be calculated during the tree merging process since the influence regarding location and amplitute of certain tree nodes are know apriory. So the upper limit is simply the sum of the re-adjusted differences of the original node and the merging partner:

\begin{equation}\label{eq:e1_up}
    \bar{e}_1\left(t, t'\right) = \frac{1}{m} \sum_{m \in \mathrm{merges}(t, t')} 2^\frac{d - m_l}{2} \left|m_x - m_y\right|
\end{equation}

Here $m_l$ stands for the level of the merged node and $m_x$ and $m_y$ for the coefficient of the original node and the merge partner. Notice that all merges are included in the sum. So even when a node gets merged after both children got merged, they children are included as well.

While the implementation is rather fast because guessing of the upper limit does not require any knowledge of the actual time series difference, it is not well suited for all applications. A compression using this metric can lead to many jumps within the decompressed time series. Also the metric tends to sacrifice huge local errors for small improvements of the overall compression rate.


\subsection{Summerized quadratic distance}
\label{ssec:algorithm:error:quadratic}

A way to put higher panilties on to huge local errors is a quadratic error function:

\begin{equation}\label{eq:e2}
    e_2\left(t, t'\right) = \frac{1}{m} \sum_{i=1}^m \left(t_i - t'_i\right)^2
\end{equation}

A drawback of this function is that the calculation requires the actual distance of the original time series and the compressed one:

\begin{equation}\label{eq:e2_up}
    \bar{e}_2\left(t, t'\right) = \frac{1}{m} \sum_{i=1}^m \bar{\delta}_i(t, t')^2
\end{equation}

Here $\bar{\delta}_i(t, t')$ is the upper limit of the distance between $t$ and $t'$ at the point in time $i$. This one can be guessed using a similar method like \autoref{eq:e1_up}.

\begin{equation}\label{eq:delta_up}
    \bar{\delta}_i\left(t, t'\right) = \sum_{m \in \mathrm{merges}(t, t')} \mathbb{1}_{m \in i} \cdot 2^{-\frac{d - m_l}{2}} \left|m_x - m_y\right|
\end{equation}

where $\mathbb{1}_{m \in i}$ describes the fact that the merge $m$ does affect the point in time $i$. Notice the minus in the exponent. Since that upper limit strictly increases with at least the amount of the real delta during each merge, the error metric will reach the user-provided limit faster than expected. So we use a hybrid approach during the merging procedure: do cheap guessing until we reach the error limit and then calculate the real error and use this as a base for the following sum-up operations. The number of recalculations increases when slowly approaching the error limit but it is still way faster than calculating the real error on every merge.


\subsection{Delta Range}
\label{ssec:algorithm:error:range}

Our last metric is an application specific one which is based on the following observation: Shifting the entire time series up or down (in terms of the $y$-Axis) does not affect the gradient. Therefore this should be allowed during the compression procedure. So we use the distance between the maximum of the delta and the minimum of the delta as an error function. Keep in mind that the delta itself is signed.

\begin{equation}\label{eq:e3}
    e_3\left(t, t'\right) = \max_{i=1}^m \left|t_i - t'_i\right| - \min_{i=1}^m \left|t_i - t'_i\right|
\end{equation}

This metric has the same drawback as the one presented in \autoref{ssec:algorithm:error:quadratic} but can guessed using a similar strategy:

\begin{equation}\label{eq:e3_up}
    \bar{e}_3\left(t, t'\right) = \max_{i=1}^m \left|\bar{\delta}_i(t, t')\right| - \min_{i=1}^m \left|\bar{\delta}_i(t, t')\right|
\end{equation}



\section{Tree as Index}
\label{sec:algorithm:asindex}

\dots



\section{Optimizations}
\label{sec:algorithm:opt}

\dots



\section{Failed Improvements}
\label{sec:algorithm:fail}

A main problem of our algorithm is its greedy nature. It merges nodes always with its nearest neighbor and therefore might make decissions that result in the fact that nodes in levels near the root might not be merged because their children do not match. We tried different approaches to solve this problem but did not succeed.


\subsection{Subtree Index}
\label{ssec:algorithm:fail:stindex}

To increase the chance that the greedy algorithm chooses merges that belong to the same subtree we tried the following method:

\begin{enumerate}
    \item select multiple nearest neighbors as candidates for a merge
    \item do this for a node which belongs to the same \num{3}-subtree (or larger ones) as well
    \item check which pairs of merges belong to the same subtree and priorize them
\end{enumerate}

This requires that we are quickly able to figure out which nodes have which parents, so we introduced a subtree and superroot index to do so. Also we need to query multiple neighbors instead of just one. Alltogether that renders the entire approach inpractical and we abandond it. Also the the compression results only improved slighly and sometimes even got worse.


\subsection{FLANN}
\label{ssec:algorithm:fail:flann}

The main goal of our algorithm is to find the best matching subtree in terms of distance. To help the greedy algorithm to archive that task we tried to use FLANN (\cite{FLANN}) to merge subtrees of size \num{3} instead of single nodes. To do so we need to ensure that the coefficients stored in the tree nodes are premultiplied like the elements in \autoref{eq:e1_up}. It turns out that the huge amount of indices required renders this method impossible to implement since it is not as memory and time efficient as plain sorted arrays. The result is that the calculation never finishes. Surprisingly the partial compression results are not even better which might be due to the fact that merging entire subtrees tends to be less aggressive compared to handling single nodes.


\subsection{Random Boosting}
\label{ssec:algorithm:fail:random}

This improvement is similar to the one presented in \autoref{ssec:algorithm:fail:stindex}. We try to find better merges by observing more neighbors. This time we tried to gather a fixed amount of neighbors and select a random one with decreasing propability bound to the rank. Then we execute a complete subtree merge. This process is repeated multiple times and the best merge is kept. We also add a merge to the set where only the nearest neighbor is used to ensure that the greedy decission never is worse due to the random elements, at least as a local outcome for that specific time series. We ended up with a heavily increased calculation time and less than \SI{5}{\percent} improvements of the compression rate.


\subsection{DTW}
\label{ssec:algorithm:fail:dtw}

One of the main limitations of the current neighborhood search is that the we are only looking for nodes which correspond to the same time span. This contridicts the later usage of the data through Dynamic Time Warping. So we tried to decouble the tree structure and subtree merging from the actual time dimensions. We limited the the search for the child nodes to a time-monotonic warping and reused the implementation of the existing DTW. For the non-leaf-nodes we did not apply any constraint since the same children policy already forces merges with the correct level and correct warping. The results of this technique were not convincing since the additional degree of freedom during the merging makes it even more unlikely to hit neighbors which belong to the same subtree. So the compression rates got worse and we dropped this approach.


\subsection{Pruning}
\label{ssec:algorithm:fail:pruning}

A possible reason for suboptimal decissions during the local node merging might be that coefficients can be quite noisy and therefore expose to many possible merges which do not really differ within their value. Because only the nearest neighbor is selected for a merge, this may result in unsane results. We tried to solve this problem by introducing additional information pruning in three variants:

\begin{enumerate}
    \item before merging
    \item during merging
    \item after merging
\end{enumerate}

The pruning zeroes low significant parts of the floating point coefficients, bit by bit, so with increasing error. So the ratio of the information loss stays approxamitly constant no matter which size the coefficient has. The idea behind this is to engage the algorithm to ignore noise within the coefficient and to have less decissions to make. On the other hand this pruning also increases the error without resulting in any compression at all and is only useful for entries that will be added to the index afterwards. Sadly the increase of the error rate overweights the desired affect of helping the greedy algorithm and therefore this approach does not have a positive effect, in all three variants.

Keep in mind that this type of pruning does not decrease the memory requirements of the tree nodes since there is no fixed cutoff but rather a individual decission how much information should be kept. So to than decrease the node size by storing a shorter representation of the coefficient you would also need to store how much information got pruned which takes so much space that it eliminates its purpose. For technical reason it is also quite difficult to store nodes with sizes not fitting into hole bytes.
